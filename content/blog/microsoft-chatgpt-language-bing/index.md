---
title: "Bing's reported ChatGPT integration might shape the future of generative A.I."
date: 2023-01-04T23:47:13+05:30
draft: false
showTableOfContents: true
showHeadingAnchors: true
---

Yesterday, [The Information reported](https://www.theinformation.com/articles/microsoft-and-openai-working-on-chatgpt-powered-bing-in-challenge-to-google) that Microsoft is planning to integrate ChatGPT, OpenAI’s breakthrough conversational AI, into Bing.

This isn’t the first time Microsoft’s indicated a desire to make concrete use of their [$1 billion investment in OpenAI](https://openai.com/blog/microsoft/) in their flagship products - they mentioned their intention to integrate DALL-E 2, OpenAI’s image generation software, in a [blog post](https://news.microsoft.com/source/features/innovation/from-hot-wheels-to-handling-content-how-brands-are-using-microsoft-ai-to-be-more-productive-and-imaginative/) last year.

It’s still only an anonymous report from ‘a person familiar with the plans’. If the claims do materialise, they’ll probably be in the form of gradual changes over the course of the next several months. This doesn’t make it any less interesting, because the exact manner in which Microsoft chooses to integrate ChatGPT into the Bing search interface, and how well their plans are executed, could be a defining moment in the course of generative AI.

Microsoft’s going to have to be careful, though. Google executives, when asked about what their plans were for launching versions of their own conversational models – currently not openly accessible – into the public sphere cited the [‘reputational risk’](https://www.theverge.com/2022/12/14/23508756/google-vs-chatgpt-ai-replace-search-reputational-risk) associated with them as a significant deterrent. And they’re probably, at the moment, right.


## The challenges facecd by generative A.I. today
There appear to be essentially three main problems associated with large language models (LLMs), like ChatGPT, today:

1. Veracity – Even within mainstream circles, interest in ChatGPT’s genuinely impressive generative abilities are balanced by a general understanding to take what it says with a grain of salt.
If you prompt it with a question that’s based on a false factual premise, it’ll still [confidently make up](https://mashable.com/article/chatgpt-amazing-wrong) an answer spewing false information.
Because of how the model works – it’s trained on a large corpus of texts and doesn’t directly refer to anything in its training set when prompted – [it also makes up false references](https://news.ycombinator.com/item?id=33841672) when asked to provide sources. In short, it has a hard time telling when it doesn’t know what it’s talking about. OpenAI CEO Sam Altman himself [cautioned](https://twitter.com/sama/status/1601731295792414720) that the model is ‘incredibly limited’ for anything ‘factual’.

2. Insensitivity – OpenAI’s added enough guardrails to attempt to prevent the model from generating anything problematic (ranging from pretending to be a human to outright hateful content) that they can sometimes restrict it from legitimate uses in conversations. However, that doesn’t mean there aren’t ways to trick it - with some clever not-so-difficult prompt engineering, people have [gotten](https://twitter.com/zswitten/status/1598380220943593472) [around](https://www.newstatesman.com/quickfire/2022/12/chatgpt-shows-ai-racism-problem) those filters.

The above two problems are probably what the Google executives were worried about, and what most of the public's attention seems to have been on so far. But I think there's another problem which people aren't talking enough about, at least outside academic and programming circles.

3. Closed-sourceness – LLMs are increasingly inaccessible to anyone who's interested in tinkering or experimenting with them. Despite their name, OpenAI in particular has been rather opaque on the exact working of their most important breakthroughs - notably, Microsoft has an [exclusive licence](https://blogs.microsoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/) to the code behind GPT-3, the underlying model behind ChatGPT. OpenAI’s also reliant on Microsoft Azure or the infrastructure powering their services and for training the model.
There’s no denying that they do need significant computiational capacity to train their developments, because of the extremely resource-intense nature of these models with billions of parameters. Even if the code for all such LLMs was made publicly availabl, its use to developers for practical fine-tuning and tweaks, and the extent to which an open-source ecosystem can be built around it, is restricted by the computational resources required to train these models. Previous developments in computer science eventually trickled down, but I’m not sure how long it’s going to take for the resources required to train DALL-E 2 – 2-4 weeks on on 256 Nvidia V100 GPUs – to become accessible to the average research lab.

---

## Why this matters

Back to Bing’s planned ChatGPT integration, then. The reason why I think this is such a crucial moment is because Microsoft is going to have to reckon with each of these challenges for this move to be successful. If they’re extremely conservative, they might end up with something that rivals [Google’s Knowledge Graph](https://support.google.com/knowledgepanel/answer/9787176?hl=en), (which is the technology behind Google knowledge panels and abstractive summaries from webpages). That’d perhaps be useful, and definitely have a positive impact on the quality of Bing search results, but considering it’s something Google’s had since long before LLMs were mainstream technology, is probably not something which Microsoft executives would see as a justification of any fraction of their significant investment in OpenAI - not to mention that it’s not the sort of application ChatGPT itself is best suited for, as it [itself has said](https://dev.to/ben/the-difference-between-chatgpt-and-gpt-3-19dh), it’s ‘not as powerful as GPT-3’ in a general context, but is better for conversational applications.

The far more exciting possibility, then, is them truly tapping into what makes ChatGPT interesting. It’s not yet known what exactly Microsoft’s vision is for ChatGPT in Bing, but my guess is it most likely involves a version of the conversational AI specifically tuned to the search engine’s results to improve veracity. This means being able to have a conversation with Bing about your query, which would undoubtedly make complex queries and multi-stage research studies significantly easier.
In some sense, this use could be what some people are already using ChatGPT for (and which they’re advised against doing for unfamiliar topics), but this time, aiming to address the pitfalls in the first problem described above – accuracy and references – by tapping into Bing’s search data. If it is successful, it would probably pressure rivals such as Google to pursue public-facing integrations of generative AI - and might mark the tipping point for mass adoption of transformer-based AI into consumer products as an active rather than passive agent.

For that to happen, though Microsoft is definitely going to have to find a way to deal with the second problem. They know all too well how letting an AI basing responses based on content on the internet can go disastrously wrong, being the same company who released a Twitter chatbot almost seven years ago which quickly became a [racist, hateful PR nightmare](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist), and probably set back the public and media perception of chatbot technology by several years.
If Microsoft is able to make ChatGPT’s conversations tuned to search results, but their filters fail to control its output, the fallout will probably be far more public than any other such incident in the field of generative AI so far - probably not just damaging the reputation of Microsoft and OpenAI, but also setting back the field and perhaps bring legislative pressure onto it while it’s still maturing.

This also means that the relationship between Microsoft and OpenAI is going to get even closer.

So far, generative models haven’t been at the centre of public-facing products for big tech companies. As soon as that happens, though, Microsoft is going to want to keep their secrets as close to their chest as they can.

Meanwhile, OpenAI’s been at the forefront of public’s relationship with AI - sure, Midjourney created a storm, but nothing like GPT (and its various forms) as well as DALL-E have. Microsoft is the only major firm which they enjoy such a close relationship with, with extensive integration across consumer products as well as significant reliance for funding. The two companies becoming closer doesn’t spell good things for the open-ness of world-shaping models like those released by OpenAI.

I don’t think OpenAI will ever completely stop existing as an independent entity. At the same time, though, Microsoft has a history of not exactly embracing open relationships with developers, from the time they [shut down Atom](https://github.blog/2022-06-08-sunsetting-atom/) or when they [came under fire](https://www.theverge.com/2022/11/8/23446821/microsoft-openai-github-copilot-class-action-lawsuit-ai-copyright-violation-training-data) for using unlicensed code from GitHub to train Copilot. I think the research company might become increasingly opaque as its ties to Microsoft increase, or at best, what remains ‘open’ will be open-source only in name, similar to how Google maintains the open-source Chromium project: contributions from independent developers are rarely accepted by the maintainers, and the repositories are essentially glorified public packages. In short, there isn’t any real open ecosystem around the software published by the company.

As competition heats up with other search providers, the incentives to allow their partner which they have a controlling say in to reveal what powers a potentially defining feature of their search experience become increasingly less. If that does happen, this might be the moment where generative models begin a downward spiral in transparency. In the long term, this makes it far more difficult for independent researchers working in labs not affiliated with any particular company to come up with new innovations, slowing down the pace at which this technology can be made less computationally intense, and better controlled by checks and balances discovered by academia.

- - - -

I don’t know if this claim will ever come to fruition, or if they do, to what extent. But I’m interested in seeing where this goes next. Because there are going to be an increasing number of inflection points like this in coming the years, as transformer models improve their reliability and sensitivity; and the needs of public, academic, and corporate entities struggle to balance their interests which will often be at odds with each other.

What generative models will be capable of, and what they’re being used for, in this decade is going to be defined by how moments like these end up affecting these competing stakeholders. The last decades of big innovations in artificial intelligence have been largely open and probably had, on the whole, a net positive impact. Moments like these are what will define their future in the decades to come.